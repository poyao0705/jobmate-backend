# Gap Analysis Data Flow – Current State

_Last updated: 2025-11-02_

This note captures the current end-to-end data flow for the skill-gap analysis pipeline (`CareerEngine.analyze_resume_vs_job`). It highlights the shapes of the payloads flowing through LangGraph, the career engine, database persistence, and the frontend API so we can migrate to a canonical schema in the follow-up steps.

## 1. Upstream Extractors

- **Resume extraction** (`LLMExtractor.extract_with_levels` cached via `_ensure_resume_cached_extract_with_levels`):
  - Returns `Dict[str, Any]` with keys:
    - `role`: optional string.
    - `skills`: dict keyed by category (`core`, `other`, etc) → list of skill dicts or plain strings.
      - Skill dict structure observed in logs/tests: `{ "name": str, "level": { "label": str, "score": float, "years": Optional[float], "confidence": Optional[float], "signals": list }, "nice_to_have": bool? }`.
    - `responsibilities`: list of str or `{ "text": str }` entries.
- **Job extraction** (same extractor, but `is_job_description=True`): identical structure with optional `nice_to_have` flag per skill.

## 2. O*NET Mapping Layer

- `OnetMapper.map_tokens` output (for skills): list of dicts like:
  ```json
  {
    "token": "Kubernetes",
    "match": {
      "skill_id": "onet.tech.kubernetes",
      "name": "Kubernetes",
      "skill_type": "skill",
      "framework": "ONET",
      "hot_tech": true,
      "in_demand": false,
      "soc_code": "13-1075.00",
      "commodity_title": "Application server software",
      "occupation": "Labor Relations Specialists",
      "text_preview": "…",
      "chunk_strategy": "recursive",
      "collection": "skills_ontology",
      "total_chunks": 1,
      "chunk_id": 0
    },
    "score": 0.82
  }
  ```
- `OnetMapper.map_tasks` output (for responsibilities): list of dicts `{ "text": <original sentence>, "match": <same metadata>, "score": float }`.
- `CareerEngine._map_with_levels` merges skill-level metadata into mapper results:
  - Adds `candidate_level` to resume-derived entries.
  - Adds `required_level` and `is_required` (bool) + optional `nice_to_have` semantics to job-derived entries.
  - Returns a single list containing both skill and task entries; tasks are differentiated by `match.skill_type != "skill"`.

## 3. Gap Analyzer Output

- `GapAnalyzer.compare(resume_map, job_map)` expects the mapped lists above and returns:
  ```json
  {
    "overall_match": 6.04,
    "matched_skills": [
      {
        "token": "Python",
        "match": { … },
        "score": 0.77,
        "candidate_level": { "label": "advanced", "score": 3.5, … },
        "required_level": { "label": "advanced", "score": 3.5, … },
        "level_delta": 0.0,
        "resume_score": 0.65,
        "is_required": true,
        "status": "meets_or_exceeds"  // populated post comparison
      },
      …
    ],
    "missing_skills": [
      {
        "token": "Kubernetes",
        "match": { … },
        "score": 0.68,
        "is_required": true,
        "is_hot_tech": true,
        "is_in_demand": false
      },
      …
    ]
  }
  ```
- The analyzer currently:
  - Filters out mapped tasks (`match.skill_type != "skill"`).
  - Computes `level_delta` (positive ⇒ underqualified) and sets `status` based on `config.score_weights.level_grace`.
  - Leaves non-skill entries untouched to avoid corrupting diagnostics.

## 4. Career Engine Post-processing

- `CareerEngine.analyze_resume_vs_job` performs additional mutations on the analyzer result:
  - `resume_skills`: derived from `resume_map` filtered for `match.skill_type == "skill"`; no deduplication beyond first occurrence.
  - `analysis_id`: database primary key (once persisted).
  - `report_md`: Markdown generated by `ReportRenderer.render(result)`.
  - Persists the following columns on `SkillGapReport`:
    - `matched_skills_json` ← `result["matched_skills"]` (list of dicts).
    - `missing_skills_json` ← `result["missing_skills"]`.
    - `weak_skills_json` ← subset of `matched_skills` where `status == "underqualified"`.
    - `resume_skills_json` ← `result["resume_skills"]` (may be `None` for legacy reports).
    - `score` ← `overall_match` (0–10 scale).

## 5. API Layer (`/gap/by-job/<job_id>`, Flask)

- Loads latest `SkillGapReport` for the user’s default resume.
- Rehydrates stored JSON (no schema validation) and performs backward-compatibility shims:
  - If `matched_skills` lack `status`, recomputes using `level_delta` and `weak_skills_json` membership.
  - Ensures `missing_skills` have `is_hot_tech` / `is_in_demand` flags by mirroring `match.hot_tech` etc.
- Constructs response payload:
  ```json
  {
    "exists": true,
    "id": 42,
    "score": 6.04,
    "matched_skills": [ … ],
    "missing_skills": [ … ],
    "weak_skills": [ … ],
    "resume_skills": [ … ],
    "report_md": "# Career Gap Analysis …"
  }
  ```
- No schema version is reported; legacy vs new content indistinguishable.

## 6. Frontend Consumption (Next.js)

- `GapGetByJobResponseSchema` (`zod`) currently allows **arbitrary** fields due to `.passthrough()` skill schema. Key expectations in `SkillGapTab`:
  - `score` on 0–10 scale (converted to percent ×10).
  - `matched_skills` & `missing_skills`: arrays containing either strings or objects. Accessed fields: `name`, `title`, `token`, `query`, `skill_name`, `match.name`, `match.skill_id`, `match.metadata.*`, `status`, `is_hot_tech`, `is_in_demand`, `level_delta`.
  - `resume_skills`: same structural assumptions as matched skills (deduplicated client-side).
  - `report_md`: Markdown string sanitized for display.
- UI derives counts/percentages and displays badges based on optional flags without guaranteed presence (multiple fall-backs per field).

## 7. Pain Points Identified

- No single typed schema; dicts mutated in-place across layers.
- Database duplicates analyzer output into multiple JSON columns (`matched`, `missing`, `weak`, `resume`) plus `score`, requiring ad-hoc hydration for legacy rows.
- API/Frontend operate on permissive structures, relying on property existence checks.
- Absence of version metadata makes migrations hard; Markdown renderer expects loosened shape.

These findings feed step 2, where we will propose a canonical, versioned contract to unify all producers and consumers.

## 8. Canonical Schema (v1.0.0)

- `GapAnalysisResult` is now the canonical Python model produced by the career engine. It carries `version`, `analysis_id`, `context`, `metrics`, and typed skill collections (`matched_skills`, `missing_skills`, `resume_skills`).
- Skill entries contain a `descriptor` block (O*NET metadata), origin/source scores, `tags` (e.g. `hot_tech`, `in_demand`), and optional `candidate_level` / `required_level` snapshots.
- `analysis_to_transport_payload()` emits an API/DB-friendly JSON payload (datetimes rendered as ISO strings) and includes `report_markdown` when available.
- New database columns:
  - `analysis_version` – stores the schema version string.
  - `analysis_json` – stores the canonical payload returned by `analysis_to_transport_payload()`.
- Legacy JSON columns remain temporarily; `build_analysis_from_legacy()` and `load_analysis_from_storage()` bridge between legacy and canonical structures.
- REST endpoints return the canonical payload under the `analysis` key while mirroring legacy fields (`score`, `matched_skills`, etc.) during the transition.
- Frontend components (e.g. `SkillGapTab`) now read from `analysis.metrics` and `analysis.*_skills`, falling back only when the canonical payload is absent.
- LangGraph state (`GapState`) keeps a hydrated `GapAnalysisResult`, improving downstream orchestration visibility.

## 9. Rollout & Compatibility

- Migration `d3f7a1c5ea12_add_analysis_json_to_skill_gap_reports` adds the new columns and backfills existing records via `build_analysis_from_legacy()`.
- For records lacking `analysis_json`, `load_analysis_from_storage()` reconstructs the canonical object from legacy columns and updates the database when possible.
- API `/gap/run` and `/gap/by-job/<id>` continue returning backward-compatible fields but clients should prefer the canonical payload (`analysis`).
- The frontend uses canonical data for scoring/badges and gracefully handles pre-migration reports.
- Legacy keys will be removed after downstream consumers migrate; until then they serve as a compatibility shield.
- LangGraph logging now sources metrics from canonical payloads when present, producing consistent telemetry.

