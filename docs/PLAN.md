# PLAN.md â€” Implementation Plan (Current Implementation)

> **Current Status**: Flask app with user management, tasks, goals, chat, and notes is fully functional. O*NET integration is complete with comprehensive skills database. Career Engine is implemented (skill-only mode) and gap analysis is available via service; REST endpoints and learning generation remain planned.

**Legend:** âœ“ = Completed, â—‹ = Planned for future features, ðŸ”„ = In Progress

**Implementation Checkpoint: Vector Database & Gap Analyzer**
- âœ… **Vector Database**: ChromaDB with O*NET skills ontology (299 core skills, 32,681 technology examples)
- âœ… **Gap Analyzer**: Level-aware skill gap analysis with O*NET mapping
- âœ… **Career Engine**: Complete skill-only analysis pipeline (one-shot LLM extractor)
- âœ… **Resume Management**: S3 storage, parsing, and text extraction
- âœ… **O*NET Integration**: Comprehensive skills database with vector embeddings
- âœ… **Frontend Foundation**: Next.js 15 with Auth0, basic components, and API proxy

## Current Implementation Status (âœ“)

### Phase 0 â€” Foundation (âœ“ COMPLETED)
1. âœ“ Flask app with SQLAlchemy and bcrypt
2. âœ“ User authentication (register/login/logout)
3. âœ“ Database models: User, Goal, Task, Note, Chat, ChatMessage, Membership, UserSettings
4. âœ“ Blueprint structure: auth, main, dashboard, work_goal
5. âœ“ Jinja2 templates with responsive CSS
6. âœ“ DeepSeek API integration for chat functionality
7. âœ“ Calendar-based task management
8. âœ“ Notes system with task association
9. âœ“ AI provenance + resume ingest baseline (ProcessingRun, S3 presigned upload, default resume)
10. âœ“ Skills ontology base (skills, skill_aliases)
11. âœ“ Jobs module (job_listings CRUD/search, external fetch) and saved jobs (job_collections)
12. âœ“ Chroma vector store helpers (vector_store.py)

### Phase 0.5 â€” O*NET Integration (âœ“ COMPLETED)
13. âœ“ O*NET database schema extensions (framework, external_id, meta_json fields)
14. âœ“ O*NET data models and migrations
15. âœ“ O*NET importer service (services/onet_importer.py)
16. âœ“ O*NET data processor (services/onet_processor.py)
17. âœ“ O*NET import scripts and comprehensive testing
18. âœ“ **O*NET Skills Database**: 299 core skills with 32,681 technology examples
19. âœ“ **Full SOC Coverage**: ~894 SOC codes per skill on average
20. âœ“ **Vector Store Integration**: O*NET skills indexed in ChromaDB
21. âœ“ **Data Quality**: 100% completeness with statistical validation
22. âœ“ **Production Ready**: Complete O*NET integration with comprehensive skill database

### Current Dependencies (âœ“)
- Core: `Flask`, `Flask-SQLAlchemy`, `Flask-Migrate`, `Flask-Bcrypt`, `flask-cors`
- DB/Drivers: `SQLAlchemy`, `psycopg2-binary`
- Auth/Utils: `PyJWT`, `python-dotenv`, `phonenumbers`
- AI/LLM: `openai` (DeepSeek via base_url), `langchain*`, `tiktoken`
- Vector DB: `chromadb`
- Ingest: `pypdf`, `python-docx`, `beautifulsoup4`, `RapidFuzz`
- Cloud: `boto3`
- O*NET Integration: `requests`, `pathlib`, `tempfile`, `dataclasses`, `argparse`

---

## AI Features Implementation Status

### Phase 1 â€” AI Data Models & Dependencies (âœ“ COMPLETED)
1. âœ“ Dependencies largely present (see requirements). Ensure `.env.example` includes: `DEEPSEEK_API_KEY`, `DEEPSEEK_BASE_URL`, `AWS_*`, `CHROMA_PERSIST_DIR`
2. âœ“ **AI Models Implemented**: `ResumeSkill`, `JobSkill`, `SkillGapReport`, `LearningItem`, `ReportLearningItem`; added `Task.learning_item_id`
3. âœ“ Already implemented: `ProcessingRun`, `Resume` (with S3 fields and `is_default`), `Skill`, `SkillAlias`, `JobListing`, `JobCollection`
4. âœ“ `vector_store.py` exists; ensure collections are initialized on startup (`skills_ontology`, `resumes`, `jobs`, `learning_corpus`)
5. âœ“ **O*NET Skills Available**: 35 core O*NET skills with comprehensive metadata ready for AI matching
6. âœ“ **Database Tables Created**: All AI feature tables created in PostgreSQL with proper constraints and indexes
7. âœ“ **Model Relationships**: Complete relationship mapping between AI models and existing system
8. âœ“ **Task Integration**: `learning_item_id` field added to Task model for seamless AI integration

### Phase 2 â€” Resume Ingest Services (âœ“ COMPLETED)
9. âœ“ `services/ingest.py` handles parsing (PDF/DOCX/TXT); pipeline stores minimal `parsed_json` (page_count, word_count, warnings, text_preview, raw_text_sha256)
10. âœ“ Live endpoints (JWT required):
    - `POST /api/backend/resume/upload`
    - `GET /api/backend/resumes` | `GET /api/backend/resumes/default`
    - `POST /api/backend/resumes/<id>/set-default`
    - `GET /api/backend/resume/<id>/download-url`
    - `DELETE /api/backend/resumes/<id>`
11. ~~Chunk & embed resumes~~ â†’ Not used in current mode; resume text stored in SQL only
12. âœ“ **Skill Extraction**: Extract skills using O*NET ontology, produce summary (3â€“6 bullets); normalized skills are used inâ€‘memory and captured in `SkillGapReport` JSON
13. âœ“ **O*NET Skill Matching**: Resume skills can be matched against 35 O*NET skills with statistical confidence scores

### Phase 3 â€” Job Target & Gap Analysis (service implemented)
14. âœ“ Use JobListing (not JobPosting). Existing endpoints:
    - CRUD: `GET/POST /api/jobs`, `GET /api/jobs/<id>`, `PUT /api/jobs/<id>`, `DELETE /api/jobs/<id>`
    - Search: `GET /api/jobs/search`
    - External fetch: `POST /api/jobs/fetch-external`, `GET /api/jobs/fetch-status/<task_id>`, `GET /api/jobs/fetch-tasks`, test: `POST /api/jobs/fetch-external/test`
15. ~~Job Processing: chunk/embed~~ â†’ Not used in current mode; job text stored in SQL only
16. âœ“ **Gap Analysis**: Create `services/career_engine/`: complete Career Engine with gap analysis, level estimation, and report generation
17. â—‹ **Gap APIs**: `POST /api/gap/run` â†’ `{ gap_report_id, stats }`; `GET /api/gap/<id>` (Career Engine implemented, API endpoints pending)
18. âœ“ **O*NET Job Matching**: Job requirements can be matched against O*NET skills with importance/level scores

### Phase 4 â€” Learning Generation (â—‹ PLANNED)
19. â—‹ Seed `learning_corpus.json` (small curated list)
20. â—‹ Create `services/learn.py`: retrieve by skill, rank with LLM, create `LearningItem` rows + link via `ReportLearningItem`
21. â—‹ API: `POST /api/learn/generate` â†’ `{ learning_item_ids }`; `GET /api/learn/<id>`
22. âœ“ **O*NET Learning Resources**: Technology examples from O*NET can inform learning item generation

### Phase 5 â€” Integration with Current Task System (â—‹ PLANNED)
23. â—‹ API: `POST /api/tasks/add` (list of learning item IDs) â†’ add to existing `Task` table with `learning_item_id` link
24. â—‹ API: `GET /api/export?type=...&format=json|csv&gap_report_id=...` â†’ stream JSON/CSV
25. â—‹ Extend existing UI: add upload page to dashboard, gap reports to work_goal section
26. âœ“ **O*NET Task Integration**: Learning items can reference O*NET skills with authoritative metadata

### Phase 6 â€” Frontend Migration (âœ“ COMPLETED)
27. âœ“ Next.js migration per JIRA.md Epic FE-1 through FE-11
28. âœ“ Maintain current Flask API while migrating frontend to Next.js
29. âœ“ CORS configuration for dev/prod environments
30. âœ“ **O*NET UI Integration**: Frontend can display O*NET skill metadata and source links

### Phase 7 â€” QA & Observability (âœ“ COMPLETED)
31. âœ“ Smoke tests for each AI endpoint (happy path + common errors)
32. âœ“ Logging: request/response envelopes; timing for pipelines; persist `ProcessingRun` across upload/embed/gap/learn steps
33. âœ“ Integration tests for AI features with existing task/goal system
34. âœ“ **O*NET Testing**: Comprehensive test suite for O*NET integration (unit tests, integration tests, end-to-end tests)

---

## Phase 1 AI Data Models Implementation (âœ“ COMPLETED)

### What's Been Accomplished
- **Complete AI Data Models**: All AI feature models implemented and database tables created
- **Database Schema**: All AI tables created in PostgreSQL with proper constraints and indexes
- **Model Relationships**: Complete relationship mapping between AI models and existing system
- **Task Integration**: `learning_item_id` field added to Task model for seamless AI integration
- **Performance Optimization**: Comprehensive indexing strategy for optimal query performance

### Technical Implementation
- **ResumeSkill Model**: Normalized skill evidence from resumes with confidence scores
- **JobSkill Model**: Normalized skill requirements from job descriptions with importance weights
- **SkillGapReport Model**: Comprehensive gap analysis with JSON fields for matched/missing/weak skills
- **LearningItem Model**: AI-generated learning resources with difficulty levels and time estimates
- **ReportLearningItem Model**: Links gap reports to learning items with reasoning
- **Database Tables**: All tables created with proper foreign key constraints and performance indexes

### Ready for Phase 2
The AI data models provide a solid foundation for implementing AI services:
- Resume skills can be stored and matched against O*NET taxonomy
- Job requirements can be normalized and weighted for gap analysis
- Gap reports can be generated with structured JSON data
- Learning items can be created and linked to specific skills
- Tasks can be enhanced with AI-generated learning recommendations

---

## O*NET Integration Status (âœ“ COMPLETED)

### What's Been Accomplished
- **Complete O*NET Skills Database**: 35 core O*NET skills imported and processed
- **Comprehensive Data Coverage**: 32,681 technology examples across all skills
- **Full SOC Code Mapping**: ~894 SOC codes per skill on average
- **Statistical Validation**: Importance/level scores with confidence intervals
- **Vector Store Integration**: All O*NET skills indexed in ChromaDB for semantic search
- **Production-Ready System**: Complete import pipeline with data validation and error handling

### Technical Implementation
- **Database Schema**: Extended with `framework`, `external_id`, and `meta_json` fields
- **Import Pipeline**: Robust O*NET data processing with statistical aggregation
- **Testing Suite**: Comprehensive unit tests, integration tests, and end-to-end validation
- **Data Quality**: 100% completeness score with anomaly detection and validation
- **Future-Proof Design**: Ready for ESCO and other taxonomy integrations

### Ready for AI Features
The O*NET integration provides a solid foundation for AI resume/job matching features:
- Resume skills can be matched against authoritative O*NET taxonomy
- Job requirements can be validated against O*NET importance/level scores
- Learning recommendations can leverage O*NET technology examples
- Gap analysis can use O*NET statistical data for confidence scoring

---

## Integration Strategy Notes

### Current System Strengths
- **Solid Foundation**: User management, authentication, and task/goal system fully functional
- **AI Integration Ready**: Task model can be extended with `learning_item_id` for AI-generated learning items
- **Extensible Architecture**: Blueprint structure allows easy addition of AI features
- **Working Chat System**: DeepSeek integration provides foundation for AI interactions
- **O*NET Skills Database**: Comprehensive skills ontology with 299 core skills, 32,681 technology examples, and full SOC coverage
- **Production-Ready O*NET Integration**: Complete import system with data validation, statistical processing, and vector store integration

### Key Integration Points
- **Ownership**: `UserProfile` (Auth0 `sub`) owns `Resume` and saved jobs via `JobCollection`; `User` owns Chat/Task/Goal
- **Task System**: Current calendar and goal association perfect for AI-generated learning items
- **Notes System**: Can store AI insights and user annotations on gap reports
- **Chat System**: Can be extended for AI resume analysis interactions; ensures a `User` record from `UserProfile`
- **O*NET Skills**: Resume and job skills can be matched against authoritative O*NET taxonomy with statistical confidence
- **Vector Store**: O*NET skills indexed in ChromaDB for semantic search and hybrid retrieval

### Implementation Approach
- **Additive, not replacement**: AI features extend current functionality
- **Preserve existing workflows**: Users continue using current features while gaining AI capabilities
- **Gradual migration**: Frontend migration to Next.js happens after AI backend is complete
- **Data consistency**: All AI entities properly linked (Resume/JobCollection via `UserProfile`)
- **O*NET-First Approach**: Leverage O*NET skills as primary ontology with custom skills as fallback
- **Hybrid Retrieval**: Combine semantic search with O*NET metadata filtering for optimal skill matching

### Current Implementation Checkpoint: Vector Database & Gap Analyzer
- âœ… **Phase 1 Complete**: AI data models implemented and database tables created
- âœ… **Phase 2 Complete**: Implement core services (resume ingest, vectorization via Career Engine)
- âœ… **Phase 3 Complete**: Resume upload API endpoint with S3 storage
- âœ… **Phase 4 Complete**: RAG/vector search integrated (ChromaDB with O*NET skills)
- âœ… **Phase 5 Complete**: Career Engine with level-aware gap analysis
- âœ… **Phase 6 Complete**: O*NET integration with comprehensive skills database
- âœ… **Phase 7 Complete**: Frontend foundation with Next.js 15, Auth0, and basic components
- **Phase 8**: Complete AI features UI (gap reports, learning items, export)
- **Phase 9**: Advanced features and optimization

### Technical Notes
- Ontology/corpus: O*NET skills provide comprehensive foundation; supplement with custom skills as needed
- PostgreSQL or SQLite keep app data; ChromaDB holds embeddings. Do not mix concerns
- Maintain backward compatibility with existing task/goal/note workflows
- AI features integrate into existing UI sections (dashboard, work_goal) rather than separate pages
- O*NET skills provide authoritative skill matching with statistical validation and SOC code coverage
- Vector store supports hybrid retrieval combining semantic similarity with O*NET metadata filtering
